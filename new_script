import pandas as pd
import numpy as np
import re
import time
import json
from datetime import datetime
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.feature_extraction.text import TfidfVectorizer
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from urllib.parse import urlparse
import os

# Download necessary NLTK data
try:
    nltk.download('vader_lexicon', quiet=True)
    nltk.download('punkt', quiet=True)
    nltk.download('stopwords', quiet=True)
except:
    print("NLTK download failed but continuing...")

# Initialize sentiment analyzer
sid = SentimentIntensityAnalyzer()

# Function to extract domain from URL
def extract_domain(url):
    try:
        domain = urlparse(url).netloc
        return domain
    except:
        return ""

# List of credible news domains
credible_domains = [
    "reuters.com", "apnews.com", "bbc.com", "bbc.co.uk", "npr.org", 
    "washingtonpost.com", "nytimes.com", "wsj.com", "economist.com",
    "hindustantimes.com", "indianexpress.com", "thehindu.com", "ndtv.com",
    "youtube.com"  # Added YouTube as it can contain credible content
]

# List of known problematic domains
problematic_domains = [
    "naturalnews.com", "infowars.com", "breitbart.com", "dailybuzzlive.com",
    "worldnewsdailyreport.com", "empirenews.net", "nationalreport.net"
]

# Function to get text complexity metrics
def get_text_complexity(text):
    if not text:
        return {"word_count": 0, "avg_word_length": 0, "sentence_count": 0}
    
    words = nltk.word_tokenize(text)
    sentences = nltk.sent_tokenize(text)
    
    word_count = len(words)
    avg_word_length = sum(len(word) for word in words) / max(1, word_count)
    sentence_count = len(sentences)
    
    return {
        "word_count": word_count,
        "avg_word_length": avg_word_length,
        "sentence_count": sentence_count
    }

# Function to load data from local JSON file
def load_local_json_data(file_path="mumbai_reddit_posts.json"):
    print(f"Loading data from local file: {file_path}")
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            posts = json.load(f)
        
        all_posts_data = []
        for post in posts:
            try:
                post_data = {
                    "id": hash(post["title"]),  # Generate an ID based on title
                    "title": post["title"],
                    "url": post["url"],
                    "domain": extract_domain(post["url"]),
                    "is_self": "reddit.com" in post["url"],
                    "selftext": "",  # JSON doesn't contain selftext
                    "score": post["score"],
                    "upvote_ratio": 0.8,  # Default value as it's not in the JSON
                    "num_comments": post["comments"],
                    "created_utc": post["created_utc"],
                    "post_age_days": (time.time() - post["created_utc"]) / (60 * 60 * 24),
                    "author": post["author"],
                    "author_created_utc": time.time() - np.random.randint(30, 1095) * 86400,  # Random author age
                    "author_age_days": np.random.randint(30, 1095),  # Random between 1 month and 3 years
                    "author_comment_karma": np.random.randint(100, 10000),  # Random karma
                    "author_link_karma": np.random.randint(50, 5000),  # Random karma
                    "author_has_verified_email": np.random.choice([True, False], p=[0.8, 0.2]),  # 80% chance of verified
                }
                
                # Analyze sentiment from title (since we don't have selftext)
                sentiment = sid.polarity_scores(post["title"])
                post_data["sentiment_neg"] = sentiment["neg"]
                post_data["sentiment_neu"] = sentiment["neu"]
                post_data["sentiment_pos"] = sentiment["pos"]
                post_data["sentiment_compound"] = sentiment["compound"]
                
                # Text complexity
                complexity = get_text_complexity(post["title"])
                post_data["word_count"] = complexity["word_count"]
                post_data["avg_word_length"] = complexity["avg_word_length"]
                post_data["sentence_count"] = complexity["sentence_count"]
                
                # Domain credibility
                domain = extract_domain(post["url"])
                post_data["domain_is_credible"] = 1 if domain in credible_domains else 0
                post_data["domain_is_problematic"] = 1 if domain in problematic_domains else 0
                
                # Comment sentiment (random since we don't have actual comments)
                num_comments = min(post["comments"], 5)
                if num_comments > 0:
                    comment_sentiments = np.random.uniform(-0.5, 0.8, num_comments)
                    post_data["avg_comment_sentiment"] = np.mean(comment_sentiments)
                    post_data["comment_sentiment_variance"] = np.var(comment_sentiments)
                else:
                    post_data["avg_comment_sentiment"] = 0
                    post_data["comment_sentiment_variance"] = 0
                
                all_posts_data.append(post_data)
            except Exception as e:
                print(f"Error processing post: {str(e)}")
                continue
        
        # Add some synthetic fake news posts to create a balanced dataset
        num_real_posts = len(all_posts_data)
        for i in range(num_real_posts):
            fake_post_data = {
                "id": f"fake{i}",
                "title": f"SHOCKING: {np.random.choice(['Mumbai', 'Maharashtra', 'India'])} {np.random.choice(['scandal', 'conspiracy', 'secret', 'cover-up'])} exposed!",
                "url": f"https://questionablenews{i}.com/article{i}",
                "domain": f"questionablenews{i}.com",
                "is_self": np.random.choice([True, False], p=[0.3, 0.7]),
                "selftext": "This incredible story has been suppressed by mainstream media!" if np.random.random() < 0.3 else "",
                "score": np.random.randint(5, 200),
                "upvote_ratio": np.random.uniform(0.4, 0.7),
                "num_comments": np.random.randint(5, 50),
                "created_utc": time.time() - np.random.randint(1, 15) * 86400,
                "post_age_days": np.random.randint(1, 15),
                "author": f"user{i+100}",
                "author_created_utc": time.time() - np.random.randint(1, 180) * 86400,
                "author_age_days": np.random.randint(1, 180),
                "author_comment_karma": np.random.randint(10, 1000),
                "author_link_karma": np.random.randint(5, 500),
                "author_has_verified_email": np.random.choice([True, False], p=[0.4, 0.6]),
                "sentiment_neg": np.random.uniform(0.1, 0.5),
                "sentiment_neu": np.random.uniform(0.3, 0.6),
                "sentiment_pos": np.random.uniform(0.1, 0.3),
                "sentiment_compound": np.random.uniform(-0.5, 0.3),
                "word_count": np.random.randint(5, 15),
                "avg_word_length": np.random.uniform(3.5, 5.5),
                "sentence_count": np.random.randint(1, 3),
                "domain_is_credible": 0,
                "domain_is_problematic": np.random.choice([0, 1], p=[0.7, 0.3]),
                "avg_comment_sentiment": np.random.uniform(-0.3, 0.2),
                "comment_sentiment_variance": np.random.uniform(0.1, 0.4)
            }
            all_posts_data.append(fake_post_data)
        
        return pd.DataFrame(all_posts_data)
    except Exception as e:
        print(f"Error loading JSON data: {str(e)}")
        return use_sample_data()  # Fallback to sample data

# Use sample data as a fallback (same as original code)
def use_sample_data():
    print("Using sample data instead...")
    
    sample_data = []
    
    # Credible post examples
    for i in range(30):
        sample_data.append({
            "id": f"cred{i}",
            "title": f"Latest economic report shows growth in manufacturing sector",
            "url": f"https://reuters.com/article{i}",
            "domain": "reuters.com",
            "is_self": False,
            "selftext": "",
            "score": np.random.randint(50, 500),
            "upvote_ratio": np.random.uniform(0.7, 0.95),
            "num_comments": np.random.randint(10, 200),
            "created_utc": time.time() - np.random.randint(1, 30) * 86400,
            "post_age_days": np.random.randint(1, 30),
            "author": f"user{i}",
            "author_created_utc": time.time() - np.random.randint(365, 1825) * 86400,
            "author_age_days": np.random.randint(365, 1825),
            "author_comment_karma": np.random.randint(1000, 50000),
            "author_link_karma": np.random.randint(500, 10000),
            "author_has_verified_email": True,
            "sentiment_neg": np.random.uniform(0, 0.2),
            "sentiment_neu": np.random.uniform(0.5, 0.8),
            "sentiment_pos": np.random.uniform(0.1, 0.4),
            "sentiment_compound": np.random.uniform(0.1, 0.8),
            "word_count": np.random.randint(50, 300),
            "avg_word_length": np.random.uniform(4.5, 6.5),
            "sentence_count": np.random.randint(5, 30),
            "domain_is_credible": 1,
            "domain_is_problematic": 0,
            "avg_comment_sentiment": np.random.uniform(0.1, 0.5),
            "comment_sentiment_variance": np.random.uniform(0.05, 0.2)
        })
    
    # Fake news post examples
    for i in range(30):
        sample_data.append({
            "id": f"fake{i}",
            "title": f"SHOCKING: You won't believe what this politician did next!",
            "url": f"https://questionablenews{i}.com/article{i}",
            "domain": f"questionablenews{i}.com",
            "is_self": (i % 3 == 0),
            "selftext": "This incredible story has been suppressed by mainstream media!" if (i % 3 == 0) else "",
            "score": np.random.randint(5, 200),
            "upvote_ratio": np.random.uniform(0.4, 0.7),
            "num_comments": np.random.randint(5, 50),
            "created_utc": time.time() - np.random.randint(1, 15) * 86400,
            "post_age_days": np.random.randint(1, 15),
            "author": f"user{i+100}",
            "author_created_utc": time.time() - np.random.randint(30, 365) * 86400,
            "author_age_days": np.random.randint(30, 365),
            "author_comment_karma": np.random.randint(10, 1000),
            "author_link_karma": np.random.randint(5, 500),
            "author_has_verified_email": (i % 2 == 0),
            "sentiment_neg": np.random.uniform(0.1, 0.5),
            "sentiment_neu": np.random.uniform(0.3, 0.6),
            "sentiment_pos": np.random.uniform(0.1, 0.3),
            "sentiment_compound": np.random.uniform(-0.5, 0.3),
            "word_count": np.random.randint(20, 150),
            "avg_word_length": np.random.uniform(3.5, 5.5),
            "sentence_count": np.random.randint(3, 15),
            "domain_is_credible": 0,
            "domain_is_problematic": (i % 4 == 0),
            "avg_comment_sentiment": np.random.uniform(-0.3, 0.2),
            "comment_sentiment_variance": np.random.uniform(0.1, 0.4)
        })
    
    return pd.DataFrame(sample_data)

# Feature engineering function
def engineer_features(df):
    print("Performing feature engineering...")
    
    # Fix the invalid escape sequence
    df['title_has_clickbait'] = df['title'].str.contains('|'.join([
        'you won\'t believe', 'shocking', 'mind blowing', 'amazing', 'unbelievable', 
        'shocking truth', 'secret', 'revealed', 'this is why', 'this will', 'won\'t believe'
    ]), case=False, regex=True).astype(int)
    
    # Use raw string for regex pattern
    df['title_has_question'] = df['title'].str.contains(r'\?').astype(int)
    df['title_is_all_caps'] = df['title'].str.isupper().astype(int)
    
    df['title_selftext_ratio'] = 0.0
    for idx, row in df[df['is_self'] == True].iterrows():
        if row['selftext'] and row['title']:
            title_words = set(nltk.word_tokenize(row['title'].lower()))
            text_words = set(nltk.word_tokenize(row['selftext'].lower()))
            if title_words and text_words:
                overlap = len(title_words.intersection(text_words))
                df.at[idx, 'title_selftext_ratio'] = overlap / len(title_words)
    
    df['post_hour'] = df['created_utc'].apply(lambda x: datetime.fromtimestamp(x).hour)
    df['post_day'] = df['created_utc'].apply(lambda x: datetime.fromtimestamp(x).weekday())
    
    df['comments_per_score'] = df['num_comments'] / (df['score'] + 1)
    
    return df

# Create synthetic labels for training
def create_synthetic_labels(df):
    print("Creating synthetic credibility labels...")
    
    credibility_score = (
        (df['domain_is_credible'] * 2) +
        (df['author_age_days'] / 365) +
        (df['author_comment_karma'] > 1000).astype(int) +
        (df['upvote_ratio'] > 0.8).astype(int) +
        (df['sentiment_compound'] > 0).astype(int) * 0.5 -
        (df['domain_is_problematic'] * 2) -
        (df['title_has_clickbait']) -
        (df['title_is_all_caps']) -
        ((df['comments_per_score'] > 1) & (df['score'] > 10)).astype(int) * 0.5
    )
    
    min_score = credibility_score.min()
    max_score = credibility_score.max()
    
    if max_score == min_score:
        df['credible'] = 1
    else:
        normalized_score = (credibility_score - min_score) / (max_score - min_score)
        df['credible'] = (normalized_score > 0.6).astype(int)
    
    return df

# Prepare data for modeling
def prepare_model_data(df):
    print("Preparing data for modeling...")
    
    features = [
        'score', 'upvote_ratio', 'num_comments', 'author_age_days', 
        'author_comment_karma', 'author_link_karma', 'sentiment_neg', 
        'sentiment_neu', 'sentiment_pos', 'sentiment_compound', 'word_count', 
        'avg_word_length', 'sentence_count', 'domain_is_credible', 
        'domain_is_problematic', 'avg_comment_sentiment', 
        'comment_sentiment_variance', 'title_has_clickbait', 'title_has_question', 
        'title_is_all_caps', 'title_selftext_ratio', 'post_hour', 'post_day',
        'comments_per_score'
    ]
    
    available_features = [f for f in features if f in df.columns]
    X = df[available_features].fillna(0)
    y = df['credible']
    
    tfidf = TfidfVectorizer(max_features=100, stop_words='english')
    title_features = tfidf.fit_transform(df['title'].fillna(''))
    X_with_text = pd.concat([X.reset_index(drop=True), pd.DataFrame(title_features.toarray())], axis=1)
    
    # Make sure all column names are strings to avoid the error
    X_with_text.columns = X_with_text.columns.astype(str)
    
    return X_with_text, y, tfidf

# Updated prediction function
def predict_credibility(post_title, post_url, post_text="", author_karma=0, author_age_days=0, rf=None, tfidf=None, X_columns=None):
    post_data = {
        "title": post_title,
        "url": post_url,
        "domain": extract_domain(post_url),
        "is_self": bool(post_text),
        "selftext": post_text,
        "score": 1,
        "upvote_ratio": 0.5,
        "num_comments": 0,
        "created_utc": time.time(),
        "post_age_days": 0,
        "author_age_days": author_age_days,
        "author_comment_karma": author_karma,
        "author_link_karma": 0,
        "title_has_clickbait": 1 if re.search('you won\'t believe|shocking|mind blowing|amazing|unbelievable|secret|revealed', post_title, re.I) else 0,
        "title_has_question": 1 if '?' in post_title else 0,
        "title_is_all_caps": 1 if post_title.isupper() else 0,
        "domain_is_credible": 1 if extract_domain(post_url) in credible_domains else 0,
        "domain_is_problematic": 1 if extract_domain(post_url) in problematic_domains else 0,
    }
    
    combined_text = post_title + " " + post_text
    sentiment = sid.polarity_scores(combined_text)
    post_data["sentiment_neg"] = sentiment["neg"]
    post_data["sentiment_neu"] = sentiment["neu"]
    post_data["sentiment_pos"] = sentiment["pos"]
    post_data["sentiment_compound"] = sentiment["compound"]
    
    complexity = get_text_complexity(combined_text)
    post_data["word_count"] = complexity["word_count"]
    post_data["avg_word_length"] = complexity["avg_word_length"]
    post_data["sentence_count"] = complexity["sentence_count"]
    
    df_new = pd.DataFrame([post_data])
    df_new = engineer_features(df_new)
    
    features = [col for col in X_columns if col in df_new.columns]
    X_new_base = df_new[features].fillna(0)
    
    title_features = tfidf.transform([post_title])
    X_new = pd.concat([X_new_base.reset_index(drop=True), pd.DataFrame(title_features.toarray())], axis=1)
    
    missing_cols = set(X_columns) - set(X_new.columns)
    for col in missing_cols:
        X_new[col] = 0
        
    X_new = X_new[X_columns]
    
    # Convert column names to strings to avoid the error
    X_new.columns = X_new.columns.astype(str)
    
    prediction = rf.predict(X_new)[0]
    probability = rf.predict_proba(X_new)[0][1]
    
    return {
        "is_credible": bool(prediction),
        "credibility_score": float(probability),
        "confidence": "high" if abs(probability - 0.5) > 0.3 else "medium" if abs(probability - 0.5) > 0.15 else "low"
    }

# Function to test the prediction function
def test_prediction(rf, tfidf, X_columns):
    if rf is None:
        try:
            from joblib import load
            rf = load('reddit_fake_news_model.joblib')
            tfidf = load('reddit_tfidf_vectorizer.joblib')
        except:
            print("Models not found. Please run the training first.")
            return
    
    test_cases = [
        {
            "title": "New study reveals significant health benefits of meditation",
            "url": "https://www.reuters.com/health/meditation-study",
            "text": "A comprehensive study of 1000 participants showed reduced stress and improved focus.",
            "karma": 5000,
            "age_days": 1095,
            "expected": "credible"
        },
        {
            "title": "SHOCKING: This one weird trick will make you lose 20 pounds overnight!",
            "url": "https://clickbait-health-news.com/miracle-diet",
            "text": "Doctors hate this! Secret weight loss method they don't want you to know!",
            "karma": 100,
            "age_days": 30,
            "expected": "not credible"
        }
    ]
    
    print("\nTesting prediction function with example posts:")
    for i, case in enumerate(test_cases):
        result = predict_credibility(
            post_title=case["title"],
            post_url=case["url"],
            post_text=case["text"],
            author_karma=case["karma"],
            author_age_days=case["age_days"],
            rf=rf,
            tfidf=tfidf,
            X_columns=X_columns
        )
        print(f"\nTest case {i+1}:")
        print(f"Title: {case['title']}")
        print(f"Expected: {case['expected']}")
        print(f"Prediction: {'credible' if result['is_credible'] else 'not credible'}")
        print(f"Confidence: {result['confidence']} ({result['credibility_score']:.2f})")

# Main execution flow
def main():
    print("Starting Reddit fake news detection model training using local data...")
    
    # Load data from the JSON file
    df = load_local_json_data()
    
    if df.empty:
        print("No data collected. Exiting.")
        return None, None, None
    
    print(f"Data collected: {len(df)} posts")
    df.to_csv("reddit_posts_raw.csv", index=False)
    
    df = engineer_features(df)
    df = create_synthetic_labels(df)
    print(f"Generated labels: {df['credible'].sum()} credible, {len(df) - df['credible'].sum()} non-credible")
    df.to_csv("reddit_posts_processed.csv", index=False)
    
    X, y, tfidf = prepare_model_data(df)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    
    print("Training Random Forest classifier...")
    rf = RandomForestClassifier(n_estimators=100, random_state=42)
    rf.fit(X_train, y_train)
    
    y_pred = rf.predict(X_test)
    print("\nRandom Forest Classification Report:")
    print(classification_report(y_test, y_pred))
    
    print("\nConfusion Matrix:")
    print(confusion_matrix(y_test, y_pred))
    
    feature_names = X.columns
    importances = rf.feature_importances_
    indices = np.argsort(importances)[::-1]
    
    print("\nFeature Ranking:")
    for f in range(min(20, X.shape[1])):
        print(f"{f+1}. {feature_names[indices[f]]} ({importances[indices[f]]:.4f})")
    
    from joblib import dump
    dump(rf, 'reddit_fake_news_model.joblib')
    dump(tfidf, 'reddit_tfidf_vectorizer.joblib')
    print("\nModel saved to 'reddit_fake_news_model.joblib'")
    
    return rf, tfidf, X.columns

# Test predictions on real Reddit posts from the JSON file
def test_mumbai_posts(rf, tfidf, X_columns):
    try:
        with open('mumbai_reddit_posts.json', 'r', encoding='utf-8') as f:
            posts = json.load(f)
        
        print("\nPredicting credibility for actual Reddit posts from Mumbai subreddit:")
        for post in posts:
            result = predict_credibility(
                post_title=post["title"],
                post_url=post["url"],
                post_text="",  # We don't have selftext in the JSON
                author_karma=1000,  # Default value
                author_age_days=365,  # Default value
                rf=rf,
                tfidf=tfidf,
                X_columns=X_columns
            )
            print(f"\nPost: {post['title']}")
            print(f"URL: {post['url']}")
            print(f"Prediction: {'credible' if result['is_credible'] else 'not credible'}")
            print(f"Confidence: {result['confidence']} ({result['credibility_score']:.2f})")
            
    except Exception as e:
        print(f"Error testing Mumbai posts: {str(e)}")

if __name__ == "__main__":
    rf, tfidf, X_columns = main()
    
    if rf is not None:
        print("\nExample usage of the prediction function:")
        example_result = predict_credibility(
            post_title="New study shows benefits of regular exercise",
            post_url="https://reuters.com/health/exercise-study",
            post_text="Researchers have found that regular exercise improves cardiovascular health.",
            author_karma=5000,
            author_age_days=730,
            rf=rf,
            tfidf=tfidf,
            X_columns=X_columns
        )
        print(f"Example prediction: {example_result}")
        
        # Test with standard test cases
        test_prediction(rf, tfidf, X_columns)
        
        # Test with actual Mumbai subreddit posts
        test_mumbai_posts(rf, tfidf, X_columns)
