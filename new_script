import praw
import pandas as pd
import numpy as np
import re
import time
from datetime import datetime
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.feature_extraction.text import TfidfVectorizer
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from urllib.parse import urlparse
import os

# Download necessary NLTK data, including punkt_tab for sentence tokenization
try:
    nltk.download('vader_lexicon', quiet=True)
    nltk.download('punkt', quiet=True)
    nltk.download('stopwords', quiet=True)
    nltk.download('punkt_tab', quiet=True)
except:
    print("NLTK download failed but continuing...")

# Initialize sentiment analyzer
sid = SentimentIntensityAnalyzer()

# Function to extract domain from URL
def extract_domain(url):
    try:
        domain = urlparse(url).netloc
        return domain
    except:
        return ""

# List of credible news domains
credible_domains = [
    "reuters.com", "apnews.com", "bbc.com", "bbc.co.uk", "npr.org", 
    "washingtonpost.com", "nytimes.com", "wsj.com", "economist.com",
    "hindustantimes.com", "indianexpress.com", "thehindu.com", "ndtv.com"
]

# List of known problematic domains
problematic_domains = [
    "naturalnews.com", "infowars.com", "breitbart.com", "dailybuzzlive.com",
    "worldnewsdailyreport.com", "empirenews.net", "nationalreport.net"
]

# Function to get text complexity metrics
def get_text_complexity(text):
    if not text:
        return {"word_count": 0, "avg_word_length": 0, "sentence_count": 0}
    
    words = nltk.word_tokenize(text)
    sentences = nltk.sent_tokenize(text)
    
    word_count = len(words)
    avg_word_length = sum(len(word) for word in words) / max(1, word_count)
    sentence_count = len(sentences)
    
    return {
        "word_count": word_count,
        "avg_word_length": avg_word_length,
        "sentence_count": sentence_count
    }

# Option 1: Use saved sample data if API fails
def use_sample_data():
    print("Using sample data instead of live Reddit API...")
    
    sample_data = []
    
    # Credible post examples
    for i in range(30):
        sample_data.append({
            "id": f"cred{i}",
            "title": f"Latest economic report shows growth in manufacturing sector",
            "url": f"https://reuters.com/article{i}",
            "domain": "reuters.com",
            "is_self": False,
            "selftext": "",
            "score": np.random.randint(50, 500),
            "upvote_ratio": np.random.uniform(0.7, 0.95),
            "num_comments": np.random.randint(10, 200),
            "created_utc": time.time() - np.random.randint(1, 30) * 86400,
            "post_age_days": np.random.randint(1, 30),
            "author": f"user{i}",
            "author_created_utc": time.time() - np.random.randint(365, 1825) * 86400,
            "author_age_days": np.random.randint(365, 1825),
            "author_comment_karma": np.random.randint(1000, 50000),
            "author_link_karma": np.random.randint(500, 10000),
            "author_has_verified_email": True,
            "sentiment_neg": np.random.uniform(0, 0.2),
            "sentiment_neu": np.random.uniform(0.5, 0.8),
            "sentiment_pos": np.random.uniform(0.1, 0.4),
            "sentiment_compound": np.random.uniform(0.1, 0.8),
            "word_count": np.random.randint(50, 300),
            "avg_word_length": np.random.uniform(4.5, 6.5),
            "sentence_count": np.random.randint(5, 30),
            "domain_is_credible": 1,
            "domain_is_problematic": 0,
            "avg_comment_sentiment": np.random.uniform(0.1, 0.5),
            "comment_sentiment_variance": np.random.uniform(0.05, 0.2)
        })
    
    # Fake news post examples
    for i in range(30):
        sample_data.append({
            "id": f"fake{i}",
            "title": f"SHOCKING: You won't believe what this politician did next!",
            "url": f"https://questionablenews{i}.com/article{i}",
            "domain": f"questionablenews{i}.com",
            "is_self": (i % 3 == 0),
            "selftext": "This incredible story has been suppressed by mainstream media!" if (i % 3 == 0) else "",
            "score": np.random.randint(5, 200),
            "upvote_ratio": np.random.uniform(0.4, 0.7),
            "num_comments": np.random.randint(5, 50),
            "created_utc": time.time() - np.random.randint(1, 15) * 86400,
            "post_age_days": np.random.randint(1, 15),
            "author": f"user{i+100}",
            "author_created_utc": time.time() - np.random.randint(30, 365) * 86400,
            "author_age_days": np.random.randint(30, 365),
            "author_comment_karma": np.random.randint(10, 1000),
            "author_link_karma": np.random.randint(5, 500),
            "author_has_verified_email": (i % 2 == 0),
            "sentiment_neg": np.random.uniform(0.1, 0.5),
            "sentiment_neu": np.random.uniform(0.3, 0.6),
            "sentiment_pos": np.random.uniform(0.1, 0.3),
            "sentiment_compound": np.random.uniform(-0.5, 0.3),
            "word_count": np.random.randint(20, 150),
            "avg_word_length": np.random.uniform(3.5, 5.5),
            "sentence_count": np.random.randint(3, 15),
            "domain_is_credible": 0,
            "domain_is_problematic": (i % 4 == 0),
            "avg_comment_sentiment": np.random.uniform(-0.3, 0.2),
            "comment_sentiment_variance": np.random.uniform(0.1, 0.4)
        })
    
    return pd.DataFrame(sample_data)

# Option 2: Try to use the Reddit API with proper error handling
def collect_reddit_data_safely():
    try:
        reddit = praw.Reddit(
            client_id="F2AyVPKAxPApH3arBONu9w",
            client_secret="06NmSk2W3V_nd2kllhnzsp1Oq3IRMA",
            user_agent="script:fakenews:1.0 (by u/Amazing-Bite-957)"
        )
        username = reddit.user.me()
        if not username:
            print("Reddit API connection failed silently - using sample data instead")
            return use_sample_data()
        
        subreddits = ["news", "worldnews", "politics", "mumbai", "science", "technology"]
        all_posts_data = []
        for subreddit_name in subreddits:
            print(f"Collecting data from r/{subreddit_name}...")
            subreddit = reddit.subreddit(subreddit_name)
            try:
                for post in subreddit.hot(limit=10):
                    try:
                        post_data = {
                            "id": post.id,
                            "title": post.title,
                            "url": post.url,
                            "domain": extract_domain(post.url),
                            "is_self": post.is_self,
                            "selftext": post.selftext,
                            "score": post.score,
                            "upvote_ratio": post.upvote_ratio,
                            "num_comments": post.num_comments,
                            "created_utc": post.created_utc,
                            "post_age_days": (time.time() - post.created_utc) / (60 * 60 * 24)
                        }
                        
                        if post.author:
                            post_data["author"] = post.author.name
                            post_data["author_created_utc"] = post.author.created_utc
                            post_data["author_age_days"] = (time.time() - post.author.created_utc) / (60 * 60 * 24)
                            post_data["author_comment_karma"] = post.author.comment_karma
                            post_data["author_link_karma"] = post.author.link_karma
                            post_data["author_has_verified_email"] = post.author.has_verified_email
                        else:
                            post_data["author"] = "[deleted]"
                            post_data["author_created_utc"] = None
                            post_data["author_age_days"] = None
                            post_data["author_comment_karma"] = None
                            post_data["author_link_karma"] = None
                            post_data["author_has_verified_email"] = None
                        
                        combined_text = post.title + " " + (post.selftext if post.selftext else "")
                        sentiment = sid.polarity_scores(combined_text)
                        post_data["sentiment_neg"] = sentiment["neg"]
                        post_data["sentiment_neu"] = sentiment["neu"]
                        post_data["sentiment_pos"] = sentiment["pos"]
                        post_data["sentiment_compound"] = sentiment["compound"]
                        
                        complexity = get_text_complexity(combined_text)
                        post_data["word_count"] = complexity["word_count"]
                        post_data["avg_word_length"] = complexity["avg_word_length"]
                        post_data["sentence_count"] = complexity["sentence_count"]
                        
                        domain = extract_domain(post.url)
                        post_data["domain_is_credible"] = 1 if domain in credible_domains else 0
                        post_data["domain_is_problematic"] = 1 if domain in problematic_domains else 0
                        
                        post.comments.replace_more(limit=0)
                        comment_sentiments = []
                        for comment in list(post.comments)[:3]:
                            if comment.body and comment.author:
                                comment_sentiment = sid.polarity_scores(comment.body)["compound"]
                                comment_sentiments.append(comment_sentiment)
                        
                        post_data["avg_comment_sentiment"] = np.mean(comment_sentiments) if comment_sentiments else 0
                        post_data["comment_sentiment_variance"] = np.var(comment_sentiments) if len(comment_sentiments) > 1 else 0
                        
                        all_posts_data.append(post_data)
                        time.sleep(2)
                    except Exception as e:
                        print(f"Error processing post: {str(e)}")
                        continue
            except Exception as e:
                print(f"Error accessing subreddit {subreddit_name}: {str(e)}")
                continue
        
        if len(all_posts_data) >= 10:
            return pd.DataFrame(all_posts_data)
        else:
            print(f"Only collected {len(all_posts_data)} posts, using sample data instead")
            return use_sample_data()
            
    except Exception as e:
        print(f"Reddit API error: {str(e)}")
        return use_sample_data()

# Feature engineering function
def engineer_features(df):
    print("Performing feature engineering...")
    
    df['title_has_clickbait'] = df['title'].str.contains('|'.join([
        'you won\'t believe', 'shocking', 'mind blowing', 'amazing', 'unbelievable', 
        'shocking truth', 'secret', 'revealed', 'this is why', 'this will', 'won\'t believe'
    ]), case=False, regex=True).astype(int)
    
    df['title_has_question'] = df['title'].str.contains('\?').astype(int)
    df['title_is_all_caps'] = df['title'].str.isupper().astype(int)
    
    df['title_selftext_ratio'] = 0.0
    for idx, row in df[df['is_self'] == True].iterrows():
        if row['selftext'] and row['title']:
            title_words = set(nltk.word_tokenize(row['title'].lower()))
            text_words = set(nltk.word_tokenize(row['selftext'].lower()))
            if title_words and text_words:
                overlap = len(title_words.intersection(text_words))
                df.at[idx, 'title_selftext_ratio'] = overlap / len(title_words)
    
    df['post_hour'] = df['created_utc'].apply(lambda x: datetime.fromtimestamp(x).hour)
    df['post_day'] = df['created_utc'].apply(lambda x: datetime.fromtimestamp(x).weekday())
    
    df['comments_per_score'] = df['num_comments'] / (df['score'] + 1)
    
    return df

# Create synthetic labels for training
def create_synthetic_labels(df):
    print("Creating synthetic credibility labels...")
    
    credibility_score = (
        (df['domain_is_credible'] * 2) +
        (df['author_age_days'] / 365) +
        (df['author_comment_karma'] > 1000).astype(int) +
        (df['upvote_ratio'] > 0.8).astype(int) +
        (df['sentiment_compound'] > 0).astype(int) * 0.5 -
        (df['domain_is_problematic'] * 2) -
        (df['title_has_clickbait']) -
        (df['title_is_all_caps']) -
        ((df['comments_per_score'] > 1) & (df['score'] > 10)).astype(int) * 0.5
    )
    
    min_score = credibility_score.min()
    max_score = credibility_score.max()
    
    if max_score == min_score:
        df['credible'] = 1
    else:
        normalized_score = (credibility_score - min_score) / (max_score - min_score)
        df['credible'] = (normalized_score > 0.6).astype(int)
    
    return df

# Prepare data for modeling
def prepare_model_data(df):
    print("Preparing data for modeling...")
    
    features = [
        'score', 'upvote_ratio', 'num_comments', 'author_age_days', 
        'author_comment_karma', 'author_link_karma', 'sentiment_neg', 
        'sentiment_neu', 'sentiment_pos', 'sentiment_compound', 'word_count', 
        'avg_word_length', 'sentence_count', 'domain_is_credible', 
        'domain_is_problematic', 'avg_comment_sentiment', 
        'comment_sentiment_variance', 'title_has_clickbait', 'title_has_question', 
        'title_is_all_caps', 'title_selftext_ratio', 'post_hour', 'post_day',
        'comments_per_score'
    ]
    
    available_features = [f for f in features if f in df.columns]
    X = df[available_features].fillna(0)
    y = df['credible']
    
    tfidf = TfidfVectorizer(max_features=100, stop_words='english')
    title_features = tfidf.fit_transform(df['title'].fillna(''))
    X_with_text = pd.concat([X.reset_index(drop=True), pd.DataFrame(title_features.toarray())], axis=1)
    
    return X_with_text, y, tfidf

# Global prediction function
def predict_credibility(post_title, post_url, post_text="", author_karma=0, author_age_days=0, rf=None, tfidf=None, X_columns=None):
    post_data = {
        "title": post_title,
        "url": post_url,
        "domain": extract_domain(post_url),
        "is_self": bool(post_text),
        "selftext": post_text,
        "score": 1,
        "upvote_ratio": 0.5,
        "num_comments": 0,
        "created_utc": time.time(),
        "post_age_days": 0,
        "author_age_days": author_age_days,
        "author_comment_karma": author_karma,
        "author_link_karma": 0,
        "title_has_clickbait": 1 if re.search('you won\'t believe|shocking|mind blowing|amazing|unbelievable|secret|revealed', post_title, re.I) else 0,
        "title_has_question": 1 if '?' in post_title else 0,
        "title_is_all_caps": 1 if post_title.isupper() else 0,
        "domain_is_credible": 1 if extract_domain(post_url) in credible_domains else 0,
        "domain_is_problematic": 1 if extract_domain(post_url) in problematic_domains else 0,
    }
    
    combined_text = post_title + " " + post_text
    sentiment = sid.polarity_scores(combined_text)
    post_data["sentiment_neg"] = sentiment["neg"]
    post_data["sentiment_neu"] = sentiment["neu"]
    post_data["sentiment_pos"] = sentiment["pos"]
    post_data["sentiment_compound"] = sentiment["compound"]
    
    complexity = get_text_complexity(combined_text)
    post_data["word_count"] = complexity["word_count"]
    post_data["avg_word_length"] = complexity["avg_word_length"]
    post_data["sentence_count"] = complexity["sentence_count"]
    
    df_new = pd.DataFrame([post_data])
    df_new = engineer_features(df_new)
    
    features = [col for col in X_columns if col in df_new.columns]
    X_new_base = df_new[features].fillna(0)
    
    title_features = tfidf.transform([post_title])
    X_new = pd.concat([X_new_base.reset_index(drop=True), pd.DataFrame(title_features.toarray())], axis=1)
    
    missing_cols = set(X_columns) - set(X_new.columns)
    for col in missing_cols:
        X_new[col] = 0
        
    X_new = X_new[X_columns]
    
    prediction = rf.predict(X_new)[0]
    probability = rf.predict_proba(X_new)[0][1]
    
    return {
        "is_credible": bool(prediction),
        "credibility_score": float(probability),
        "confidence": "high" if abs(probability - 0.5) > 0.3 else "medium" if abs(probability - 0.5) > 0.15 else "low"
    }

# Function to test the prediction function
def test_prediction(rf, tfidf, X_columns):
    from joblib import load
    if rf is None:
        try:
            rf = load('reddit_fake_news_model.joblib')
            tfidf = load('reddit_tfidf_vectorizer.joblib')
        except:
            print("Models not found. Please run the training first.")
            return
    
    test_cases = [
        {
            "title": "New study reveals significant health benefits of meditation",
            "url": "https://www.reuters.com/health/meditation-study",
            "text": "A comprehensive study of 1000 participants showed reduced stress and improved focus.",
            "karma": 5000,
            "age_days": 1095,
            "expected": "credible"
        },
        {
            "title": "SHOCKING: This one weird trick will make you lose 20 pounds overnight!",
            "url": "https://clickbait-health-news.com/miracle-diet",
            "text": "Doctors hate this! Secret weight loss method they don't want you to know!",
            "karma": 100,
            "age_days": 30,
            "expected": "not credible"
        }
    ]
    
    print("\nTesting prediction function with example posts:")
    for i, case in enumerate(test_cases):
        result = predict_credibility(
            post_title=case["title"],
            post_url=case["url"],
            post_text=case["text"],
            author_karma=case["karma"],
            author_age_days=case["age_days"],
            rf=rf,
            tfidf=tfidf,
            X_columns=X_columns
        )
        print(f"\nTest case {i+1}:")
        print(f"Title: {case['title']}")
        print(f"Expected: {case['expected']}")
        print(f"Prediction: {'credible' if result['is_credible'] else 'not credible'}")
        print(f"Confidence: {result['confidence']} ({result['credibility_score']:.2f})")

# Main execution flow
def main():
    print("Starting Reddit fake news detection model training...")
    df = collect_reddit_data_safely()
    if df.empty:
        print("No data collected. Exiting.")
        return None, None, None
    
    print(f"Data collected: {len(df)} posts")
    df.to_csv("reddit_posts_raw.csv", index=False)
    
    df = engineer_features(df)
    df = create_synthetic_labels(df)
    print(f"Generated labels: {df['credible'].sum()} credible, {len(df) - df['credible'].sum()} non-credible")
    df.to_csv("reddit_posts_processed.csv", index=False)
    
    X, y, tfidf = prepare_model_data(df)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    
    print("Training Random Forest classifier...")
    rf = RandomForestClassifier(n_estimators=100, random_state=42)
    X_train.columns = X_train.columns.astype(str)
    X_test.columns = X_test.columns.astype(str)

    rf.fit(X_train, y_train)
    
    y_pred = rf.predict(X_test)
    print("\nRandom Forest Classification Report:")
    print(classification_report(y_test, y_pred))
    
    print("\nConfusion Matrix:")
    print(confusion_matrix(y_test, y_pred))
    
    feature_names = X.columns
    importances = rf.feature_importances_
    indices = np.argsort(importances)[::-1]
    
    print("\nFeature Ranking:")
    for f in range(min(20, X.shape[1])):
        print(f"{f+1}. {feature_names[indices[f]]} ({importances[indices[f]]:.4f})")
    
    from joblib import dump
    dump(rf, 'reddit_fake_news_model.joblib')
    dump(tfidf, 'reddit_tfidf_vectorizer.joblib')
    print("\nModel saved to 'reddit_fake_news_model.joblib'")
    
    return rf, tfidf, X.columns

if __name__ == "__main__":
    rf, tfidf, X_columns = main()
    
    if rf is not None:
        print("\nExample usage of the prediction function:")
        example_result = predict_credibility(
            post_title="New study shows benefits of regular exercise",
            post_url="https://reuters.com/health/exercise-study",
            post_text="Researchers have found that regular exercise improves cardiovascular health.",
            author_karma=5000,
            author_age_days=730,
            rf=rf,
            tfidf=tfidf,
            X_columns=X_columns
        )
        print(f"Example prediction: {example_result}")
        
        # Test the prediction function with test cases
        test_prediction(rf, tfidf, X_columns)
